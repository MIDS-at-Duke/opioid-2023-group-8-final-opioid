{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse source data to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All files has been parsed and upload to the Intermediate_Files Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as raw data file is large for github, download data to local\n",
    "# adjust the path to your local path\n",
    "path = \"../00_Source_Data/arcos_all_washpost.tsv\"\n",
    "csv_chunk = pd.read_table(path, chunksize=50_000, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Washington = []\n",
    "for i, chunk in enumerate(csv_chunk):\n",
    "    append_chunk = chunk.loc[chunk[\"BUYER_STATE\"] == \"WA\"]\n",
    "    Washington.append(append_chunk)\n",
    "data_selected_Washington = pd.concat(Washington)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as raw data file is large for github, download data to local\n",
    "# adjust the path to your local path\n",
    "path = \"../00_Source_Data/arcos_all_washpost.tsv\"\n",
    "csv_chunk = pd.read_table(path, chunksize=50_000, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texas = []\n",
    "for i, chunk in enumerate(csv_chunk):\n",
    "    append_chunk = chunk.loc[chunk[\"BUYER_STATE\"] == \"TX\"]\n",
    "    Texas.append(append_chunk)\n",
    "data_selected_Texas = pd.concat(Texas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as raw data file is large for github, download data to local\n",
    "# adjust the path to your local path\n",
    "path = \"../00_Source_Data/arcos_all_washpost.tsv\"\n",
    "csv_chunk = pd.read_table(path, chunksize=50_000, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Florida = []\n",
    "for i, chunk in enumerate(csv_chunk):\n",
    "    append_chunk = chunk.loc[chunk[\"BUYER_STATE\"] == \"FL\"]\n",
    "    Florida.append(append_chunk)\n",
    "data_selected_Florida = pd.concat(Florida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat three parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdList = [\n",
    "    data_selected_Washington,\n",
    "    data_selected_Texas,\n",
    "    data_selected_Florida,\n",
    "]  # List of your dataframes\n",
    "df = pd.concat(pdList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset, calculate dosage, and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df[\n",
    "    [\n",
    "        \"BUYER_STATE\",\n",
    "        \"BUYER_COUNTY\",\n",
    "        \"TRANSACTION_DATE\",\n",
    "        \"CALC_BASE_WT_IN_GM\",\n",
    "        \"DOSAGE_UNIT\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create year variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'transaction_date' column to datetime format\n",
    "subset_df[\"TRANSACTION_DATE\"] = pd.to_datetime(subset_df[\"TRANSACTION_DATE\"])\n",
    "\n",
    "# Extract the year and create a new column 'transaction_year'\n",
    "subset_df[\"transaction_year\"] = subset_df[\"TRANSACTION_DATE\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.drop(columns=[\"TRANSACTION_DATE\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dosage total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df[\"DOSAGE_TOTAL\"] = subset_df[\"CALC_BASE_WT_IN_GM\"] * subset_df[\"DOSAGE_UNIT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dosage = subset_df.drop([\"CALC_BASE_WT_IN_GM\", \"DOSAGE_UNIT\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dosage[\"DOSAGE_TOTAL\"] = dosage.groupby(\n",
    "    [\"BUYER_STATE\", \"BUYER_COUNTY\", \"transaction_year\"]\n",
    ")[\"DOSAGE_TOTAL\"].transform(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dosage = dosage.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.641000e+03\n",
       "mean     7.604927e+07\n",
       "std      4.286634e+08\n",
       "min      1.059450e+00\n",
       "25%      1.282726e+06\n",
       "50%      5.744658e+06\n",
       "75%      3.028602e+07\n",
       "max      9.717061e+09\n",
       "Name: DOSAGE_TOTAL, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dosage[\"DOSAGE_TOTAL\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_year\n",
       "2012    335\n",
       "2006    335\n",
       "2013    333\n",
       "2007    333\n",
       "2011    332\n",
       "2014    332\n",
       "2019    331\n",
       "2016    331\n",
       "2018    331\n",
       "2009    331\n",
       "2010    331\n",
       "2015    330\n",
       "2017    330\n",
       "2008    328\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dosage[\"transaction_year\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dosage.to_parquet(\"../20_Intermediate_Files/Dosage.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
